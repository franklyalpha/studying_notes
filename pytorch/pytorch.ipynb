{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizing importing torch takes much longer time than expected, thus should always run this line before moving down. \n",
    "import torch\n",
    "import torchvision\n",
    "import torchtext\n",
    "import torch.nn as nn # -- package\n",
    "import torch.nn.functional as F # -- package"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a head start for all pytorch functionalities. Useful for providing a basic understanding. But for more advanced usages, it's always recommended to visit official pytorch official API guide: <br>\n",
    "https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor related: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.zeros((dimension), dtype=(default)torch.int32)\n",
    "\n",
    "<br>\n",
    "\n",
    "torch.ones((dimension), dtype=(default)torch.int32)\n",
    "\n",
    "<br>\n",
    "\n",
    "torch.rand((dimension)); \n",
    "\n",
    "<br>\n",
    "\n",
    "torch.randn((dimension)) -> element-wisely normalized between 0, 1\n",
    "> Useful for applying reparameterization trick for weights of Bayesian Neural Network\n",
    "\n",
    "<br>\n",
    "\n",
    "torch.tensor(input, dtype=torch.float32)\n",
    "- \"input\" must have consistent shape and can only hold either integers or floats. Can be Numpy array or other python data structure satisfying the condition. \n",
    "- \"dtype\": refer to this list for a complete set of datatypes: https://pytorch.org/docs/stable/tensors.html\n",
    "\n",
    "<br>\n",
    "\n",
    "torch.range(start=0, end, step=1) \n",
    "> Similar to numpy.arange(). \"start\" and \"end\" are INCLUSIVE!!!\n",
    "\n",
    "<br>\n",
    "\n",
    "torch.linspace(start, end, steps)\n",
    "> Create a 1D tensor with each element being linearly spaced. \n",
    "\n",
    "<br>\n",
    "\n",
    "torch.logspace(start, end, steps, base=10.0)\n",
    "> Create a 1D tensor with each element bing log-spaced. <br>\n",
    "e.g: torch.logspace(1, 3, step=3) -> 10, 100, 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10.,  100., 1000.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logspace(1, 3, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.flatten(input, start_dim=0, end_dim=-1) -> Tenser\n",
    "> flatten the data from \"input\" into 1D array, by specifying the start and end dimensions. \n",
    "\n",
    "<br>\n",
    "\n",
    "torch.reshape(input, shape_tuple) / Tensor.reshape(shape_tuple) -> Tensor\n",
    "> the new shape must be able to contain all elements from \"input\". \n",
    "\n",
    "<br>\n",
    "\n",
    "torch.cat(tensor_tuple, dim=0) -> Tensor\n",
    "> Concaatenates given tuples of tensors along \"dim\". <br> All tensors must have same shape except the concatenated dimension. \n",
    "\n",
    "<br>\n",
    "\n",
    "torch.permute(input, new_dim)/Tensor.permute(new_dim) -> Tensor\n",
    "> permute the dimension according to \"new_dim\" tuple.\n",
    "- new_dim: all integers in \"new_dim\" must be between 0 and 'len(input.size)', and must be unique. \n",
    "\n",
    "<br>\n",
    "\n",
    "torch.swapaxis/transpose(input, axis0, axis1) -> Tensor\n",
    "> swap two axes specified by \"axis0\" and \"axis1\" in \"input\".  \n",
    "\n",
    "<br>\n",
    "\n",
    "torch.gather(input, dim, index) -> Tensor\n",
    "> Extract elements from \"inputs\" according to \"index\" along \"dim\". \n",
    "- dim: cannot exceed \"len(input.size())\"\n",
    "- index: cannot exceed \"input.size(dim)\". Similar role to indices in \"numpy.indices()\"\n",
    "> See below code demo for how this method is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[1, 100], [3, 400]])\n",
    "print(torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]])))\n",
    "print(torch.gather(t, 0, torch.tensor([[0, 0], [1, 0]])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor.clone() -> Tensor\n",
    "> Returns a copy of Tensor\n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.mask_fill_(mask, value) -> Union[None, Tensor]\n",
    "> Fill the given Tensor with \"value\" at \"mask\" locations\n",
    "- “fill_” indicates in-place(can be removed to return Tensor), <br> \"fill\" indicates returning a new Tensor. \n",
    "- mask: BOOLEAN Tensor, must have shape boardcastable with “Tensor”; \n",
    "- value: when mask indice is “True”, “value” will replace the value; is a scalar\n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.size(dim=None) -> Union[tuple, int]\n",
    "> Returns size of the Tensor; \n",
    "- dim: when “dim” is specified, a scalar will be returned (the size for the corresponding dimension);\n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.split(split_size_or_sections, dim=0) -> List[Tensor]:\n",
    "> split given \"Tensor\" into a set of smaller Tensors along \"dim\". \n",
    "- Split_size_or_sections: int/list[int], indicating the size of each chunk after splitting;\n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.squeeze(dim=None) -> Tensor: \n",
    "> Removes all dimensions in Tensor with size “1”; -> “A x 1 x B” will be changed to “A x B”\n",
    "- dim: when specified, if the size of “dim” is 1, will be removed; \n",
    "    > OTHERWISE size remain UNCHANGED\n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.unsqueeze(dim) -> Tensor:\n",
    "> Inserts a new dimension with size “1” at given “dim”;\n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.view(dim1_size, dim2_size, ….) -> Tensor\n",
    "> equivalent to reshaping operations. <br> \n",
    "The product of each new dimension’s size must match with number of elements in “Tensor”. \n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.to(args) -> Tensor:\n",
    "> Return a Tensor with new properties. Commonly used for putting a Tensor into a device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "new_tensor = torch.zeros(3)\n",
    "new_tensor.to(device)\n",
    "print(new_tensor.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor.repeat((repeat_count)) -> Tensor\n",
    "> Repeat a given tensor according to \"repeat_count\". \n",
    "- repeat count: a tuple showing the counts of repetitions along given dimensions. \n",
    "> Intuition for understanding this method: <br>\n",
    "&emsp;    Treat whole \"Tensor\" as a scalar, and repetition would be just repeating this scalar to a larger tensor of shape \"repeat_count\". <br>\n",
    "&emsp;    Then replacing those scalars into \"Tensor\" would give resulting tensor. \n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.scatter_(dim, index, src) -> None\n",
    "> Replacing elements of \"Tensor\" at \"index\" along \"dim\" with elements from \"src\", in-place operation. \n",
    "- \"index\": is a tensor of integers, each element represents the index at a given dimension (can refer to \"np.indices\" for a better understanding). \n",
    "- \"src\": have the same dimensionality as \"index\", and must have lengths at least as high as \"index\"'s largest elements. \n",
    "    > \"index\"'s elements must not exceed the dimension of \"src\" or result in \"index out of range problem\"\n",
    "> See below code demo for a better understanding. \n",
    "<center><img src=\"reference_img/scatter_ref.jpg\" width=800></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo of tensor.scatter_, in-place version of tensor.scatter\n",
    "test_tensor = torch.arange(40).reshape(2, 5, 4)\n",
    "source_tensor = torch.arange(100, 132).reshape(2, 4, 4)\n",
    "ind = torch.tensor([[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], \n",
    "[[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]])\n",
    "# print(test_tensor.scatter(2, ind, source_tensor)) # this gives a complete replacement of elements for given 4 rows\n",
    "print(test_tensor.scatter(1, ind, source_tensor))\n",
    "# print(test_tensor.scatter(1, ind[:, :, :], source_tensor))\n",
    "print(test_tensor.scatter(1, ind[:, 3:], source_tensor))\n",
    "print(test_tensor.scatter(1, ind[:, 2:], source_tensor))\n",
    "# above two line's experiment shows, given the length is constrained, the indexing of sublists in super list also affects the elements being filled. \n",
    "# may use these two lines(and more custom code) to understand filling methods shown in attached image of Tensor.scatter_. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tensor.expand(new_size) -> Tensor\n",
    "> Expand dimensions with size 1 to a larger sized tensor. \n",
    "- new_size: a tuple representing expanded tensor's size. Need to be consistent with \"Tensor.size()\" or \"-1\" to indicate no change, except at dimensions with size 1. \n",
    "> Note: the operation is suggested to apply to \"cloned\" tensor, or can lead to problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo of method Tensor.expand(new_size)\n",
    "\n",
    "test_tensor = torch.arange(32).reshape(2, 4, 1, 4)\n",
    "res = test_tensor.clone().expand(2, 4, 5, 4)\n",
    "print(res.size())\n",
    "# res2 = test_tensor.expand(2, 4, 1, 5) # this is a demo of non-singleton dimension, will result in error\n",
    "res3 = test_tensor.clone().expand(1, 1, 2, 4, 2, 4) # this demos appending new dimensions at front, and matching singleton dimension locations. \n",
    "print(res3.size()) \n",
    "res4 = res3.clone().expand(2, 2, 2, 4, 2, 4)\n",
    "print(res4.size()) # this demos expanding multiple singleton dimensions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor.addr(vec1, vec2, beta=1, alpha=1) -> Tensor; \n",
    "> An addition with a vector-OUTER-product and input(vector product will be a matrix), following the below formula: \n",
    "\n",
    "<center><img src=\"reference_img/addr.png\" width=400></center>\n",
    "\n",
    "- \"input\" is Tensor itself. \n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.bmm(second-matrix, *) -> Tensor\n",
    "> Perform a matrix-matrix product.\n",
    "<center><img src=\"reference_img/tensor_bmm.jpg\" width=800></center>\n",
    "\n",
    "- \"second-matrix\" must have shapes following the criterion of matrix product. \n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.add(other) -> Tensor: \n",
    "> Add \"Tensor\" and \"other\" following broadcasting rules. \n",
    "- \"other\" must have shape boardcastable with Tensor.\n",
    "    > Shape boardcasting: other’s shape must be boardcastable with Tensor’s [LAST few dimensions]\n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.cross(other, dim=None, ) -> Tensor\n",
    "> Returns cross product of vectors from Tensor and other, applied on “dim”; \n",
    "<br> &emsp; To avoid problems, need to ensure the vectors size are consistent\n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.det() -> Tensor: \n",
    "> Returns determinant of \"Tensor\". \n",
    "- Tensor: must have last two dimensions being a square matrix. Operation only applies to last two dimensions. \n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.diag(diagonal=0) -> Tensor:\n",
    "> Extract diagonal elements of \"Tensor\". \n",
    "- \"Tensor\" can be either 1D or 2D; <br> &emsp;if 1D: returns a new square matrix tensor with “Tensor” elements on diagonal; <br> &emsp;if 2D tensor, will return diagonal elements on “Tensor”. \n",
    "- “diagonal”: controls shifting of diagonal line; \n",
    "<br> &emsp;positive: the diagonal line is drawn above main diagonal; \n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.triu(diagonal=0) -> Tensor: \n",
    "> Returns upper triangle of 2D “Tensor”. \n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.matmul(other) -> Tensor:\n",
    "> Performs matrix-vector products, requiring size of “Tensor” and “other” to be multipliable; <br> &emsp;\n",
    "Order matters: returns \"Tensor @ other\"\n",
    "\n",
    "\n",
    "> This function is omniscient, and can automatically determine which operation to adopt based on input vector; more details refer to API; <br>\n",
    "When dimension is larger than 2, batch_matrix products are conducted; \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "torch.linalg.eig(A) -> tuple[Tensor, Tensor]\n",
    "> Computes eigenvalue decomposition of “A”, a DIAGONIZABLE square matrix; \n",
    "<center><img src=\"reference_img/diagonalizable_mat.png\" width=200></center>\n",
    "\n",
    "> Returns: tuple[eigenvalues, eigenvectors] <br> &emsp;\n",
    "Realize: COLUMNS (dim=1) are corresponding eigenvectors!!!(See below image, where columns, grouped by red circles, represents one eigenvector) <br> &emsp;\n",
    "Final eigenvector matrix is NORMALIZED to 1!!! \n",
    "<center><img src=\"reference_img/eigenvector_sample.png\" width=600></center>\n",
    "\n",
    "- \"A\": has shape (*, N, N), and the method will be applied to only last two dimensions of A.  \n",
    "\n",
    "<br>\n",
    "\n",
    "torch.linalg.norm(A, ord=None, dim=None, ) -> Tensor:\n",
    "> Computes vector norm or matrix norm; \n",
    "- \"ord\": type of matrix norm to choose, if A is a matrix; (refer to API for specific types; )\n",
    "\t> Common Ord: Frobenius norm\n",
    "- dim: If “dim” is an int, vector norm will be calculated; if 2-tuple, will calculate matrix norm.\n",
    "\n",
    "<br>\n",
    "\n",
    "torch.linalg.inv(A) -> Tensor: \n",
    "> return inverse of matrix A; requiring last two dimension of \"A\" be squared and invertible.\n",
    "\n",
    "torch.svd(A) -> tuple(Tensor, Tensor, Tensor)\n",
    "> Computes the singular-value-decomposition of matrix A; \n",
    "\n",
    "\n",
    "> Returns a tuple, (U, S, V) where “U” and “V” are square matrices, and “S” is a diagonal matrix; <br>\n",
    "\n",
    "\n",
    ">Singular value decomposition can be used to determine whether a filter for convolution is separable. \n",
    "<!-- (see csc420 lecture 2 notes) -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: each functions listed below has two formats: with \"_\" at end indicates in-place operation, or a new tensor is returned. \n",
    "\n",
    "Tensor.asin_()\n",
    "> return inverse of sine function; e.g, finding the degree given a sine value\n",
    "\n",
    "> acos, atan\n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.sin()\n",
    "> cos, tan, sinh, cosh, tanh, arcsinh, arccosh, arctanh\n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.ceil(), Tensor.floor()\n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.exp()\n",
    "> Exponential with base \"e\", and power as \"Tensor\". Element-wise operation. \n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.sum(dim=None) -> Tensor\n",
    "- dim: if \"None\", will flatten and sum. \n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.logcumsumexp(dim) -> Tensor\n",
    "> Returns log-sum-exponential calculation of a tensor along \"dim\". \n",
    "<center><img src=\"reference_img/log_sum_exp.png\" width=400></center>\n",
    "\n",
    "> The summation result is cumulative (see \"numpy.cumsum\" for more reference on cumulation)\n",
    "\n",
    "> Using this method for finding a stable denominator when calculating softmax or other non-linear activation functions'results. \n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.logsumexp(dim) -> Tensor\n",
    "> Equivalent to logcumsumexp except the result keeps only the sum over entire dimension. (Final dimension will be reduced)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor.amax/amin(dim) -> Tensor\n",
    "> Take maximum/minimum of Tensor along “dim”; \n",
    "\n",
    "> Returns VALUE\n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.argmax/argmin(dim) -> Tensor\n",
    "> Take maximum/minimum of Tensor along “dim”; \n",
    "\n",
    "> Returns INDICES; \n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.mean/median/mode/sum(dim=None) -> Tensor\n",
    "- \"dim\" can also be a tuple of ints, meaning it will be calculated in multiple dimensions. See below demo\n",
    "\n",
    "<br>\n",
    "\n",
    "Tensor.cov() -> Tensor\n",
    "> Gives covariance of given Tensor, NOT COVARIANCE MATRIX!!!\n",
    "- Tensor: 2D tensor, where first dimension/dim=0 gives each sample vector, and second dimension/dim=1 gives values from all sample vectors at that index. \n",
    "<center><img src=\"reference_img/cov.png\" width=400></center>\n",
    "\n",
    "> Note: $\\bar{x}$ and $\\bar{y}$ in above images are means of each sample vector $X$, $Y$ respectively. \n",
    "\n",
    "> Refer to API for biased output information; (parameter: correction, fweights, aweights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.5000, 23.5000, 39.5000])\n",
      "tensor([[ 1.5000,  5.5000,  9.5000, 13.5000],\n",
      "        [17.5000, 21.5000, 25.5000, 29.5000],\n",
      "        [33.5000, 37.5000, 41.5000, 45.5000]])\n"
     ]
    }
   ],
   "source": [
    "# Demo of Tensor.mean with multiple dimensions\n",
    "test_tensor = torch.arange(48, dtype=torch.float32).reshape(3, 4, 4)\n",
    "print(test_tensor.mean((1, 2)))\n",
    "print(test_tensor.mean((2))) # comparing the result shows the mean is calcuated and shown for the foremost dimension, by finding means of later dimensions' calculation results. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Related: \n",
    "This note only contains commonly used methods, and only listed common parameters adopted. For detailed usage which might be uncommon, a visit to API is still required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # -- package\n",
    "import torch.nn.functional as F # -- package"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear(in_d, out_d)(input) -> output\n",
    "> The most fundamental fully-connected layers, represented by a linear matrix with shape $out\\_ d \\times in\\_ d$\n",
    "- input: has shape (..., in_d)\n",
    "- output: has shape (..., out_d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layers: \n",
    "Fundamental vision tasks operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, padding_mode=’zeros’, ...)(input) -> output\n",
    "> Defines a 2D convolutional layer; <br>\n",
    "<!-- Groups: groups input, each grouped input will have independent weights, having same output channel number, but input channel is equivalent to number of input channels in the group. Group must be divisible by input_channel to perform actions; https://iksinc.online/2020/05/10/groups-parameter-of-the-convolution-layer/  -->\n",
    "\"input\": has shape $(N,C\\_in, H, W)$ or $(C\\_in, H, W)$; output has shape $(N, C\\_out, H\\_out, W\\_out)$ or $(C\\_out, H\\_out, W\\_out)$, where $H\\_out, W\\_out$ are defined as below: \n",
    "\n",
    "\n",
    "<center><img src=\"reference_img/convoutshape.png\" width=500></center>\n",
    "\n",
    "- in_channels: $in\\_C$ of \"input\"\n",
    "- out_channels: $out\\_C$ of \"output\"\n",
    "- stride: distance between any two filters\n",
    "- padding: extra space added to \"input\"'s boundary\n",
    "- dilation: distance between each kernel's elements. \n",
    "\n",
    "> Refer to this visualization: https://ezyang.github.io/convolution-visualizer/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "Used in conjunction with convolutional layers to restore resolution for vision tasks. <br>\n",
    "Interpolation (nn.Functional.interpolate) is an alternative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.MaxPool2d(Kernel_size, stride=None,  padding=0, dilation=1, return_indices=False)<br> &emsp;(N, C, H_in, W_in)/(C, H_in, W_in) -> (N, C, H_out, W_out) / (C, H_out, W_out)\n",
    "> Pool the resulting tensor with given configurations; <br>\n",
    "&emsp;Kernels can be overlapping if stride is set less than kernel size\n",
    "<br> &emsp;Return_indices=True: indices returned could be useful for MaxUnpool2d below\n",
    "\n",
    "> Output configuration: \n",
    "<center><img src=\"reference_img/maxpoolout.jpg\" width=500></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<!-- nn.MaxUnpool2d(kernel_size, stride, padding)(input_tensor, max_indices_for_input) (NCHW/CHW)\n",
    "\ta PARTIAL reverse of max pooling where non-max values are all set to zero. \n",
    "Max’s indices for each kernel: can be generated by MaxPool2d; If possible can perform a maxpool on masked array to acquire indices -->\n",
    "<!-- \n",
    "<br> -->\n",
    "\n",
    "nn.AvgPool2d(kernel, stride=None, padding=0, count_include_pad=True, divisor_override=None)<br> &emsp;(N, C, H_in, W_in)/(C, H_in, W_in) -> (N, C, H_out, W_out) / (C, H_out, W_out)\n",
    "> Instead of extracting max, this pooling outputs average of values for each kernel. \n",
    "- count_include_pad: average calculation will also count zero paddings in this kernel, affects denominator. \n",
    "- divisor_override: will change divisor when calculating average (specify a number)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Layers: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.ReflectionPad2d(padding: int/tuple_of_ints)(input) -> Tensor\n",
    "> A demonstration of result is shown below. <br> Pay attention to the reflective axis!\n",
    "<br> Original input is at the center of resulting padded tensor\n",
    "\n",
    "- “padding”: must have size less than “min(H, W)” of input “tensor”!!!\n",
    "- input: has shape: (NCHW)/(CHW)\n",
    "<center><img src=\"reference_img/reflectpad.png\" width=300></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.ReplicationPad2d(padding)(input) -> Tensor\n",
    "> Replication repeats values CLOSEST to it. See demo below for a visualization\n",
    "<center><img src=\"reference_img/replicatpad.png\" width=300></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.ZeroPad2d(padding)(input) -> Tensor\n",
    "> pads extra spaces using zero only\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.ConstantPad2d(padding, value)(input) -> Tensor\n",
    "> Similar to zero padding, but the value can also be other constants not zero. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo of reflection padding and replication padding\n",
    "\n",
    "test_tensor = torch.Tensor(torch.arange(27, dtype=torch.float32).reshape(1, 3, 3, 3)) # NCHW\n",
    "print(test_tensor[0][0])\n",
    "print(nn.ReflectionPad2d(2)(test_tensor)[0][0])\n",
    "print(nn.ReflectionPad2d(2)(test_tensor).shape)\n",
    "print(nn.ReplicationPad2d(2)(test_tensor)[0][0])\n",
    "print(nn.ReplicationPad2d(2)(test_tensor).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Layers: \n",
    "Gradient exploded or unstable? Inconsistent units and scale? Ask Normalization for help!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"reference_img/normdiag.png\" width=800></center><center> <img src=\"reference_img/normform.png\" width=300></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.BatchNorm2d(num_features, eps=1e-05, affine=True, track_running_stats=True)(input) -> output\n",
    "> Normalize all inputs along 'batch dimension', applied 'channel-wise'. \n",
    "- num_features: “C” in above graph(don’t confuse with width/height of input data!!!)\n",
    "- eps: epsilon when calculating normalization configurations (in denominator, to avoid denominator divide by zero error)\n",
    "- affine: the parameter “gamma” and “beta” in normalization formula would be learnable\n",
    "<!-- track_running_stats;  -->\n",
    "- input: has shape [NCHW], 4D tensor\n",
    "- output: has same shape as \"input\"\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)(input) -> output\n",
    "> normalized_shape: a tuple of integers; length of tuple determine how many dimensions COUNTED FROM LAST would be normalized. <br>\n",
    "&emsp; Norms for those dimensions are calculated INDEPENDENTLY; i.e, \"Tensor.mean((-2, -1))\". <br>\n",
    "&emsp;&emsp; Note this normalization is not specific for 2D. \n",
    "\n",
    "> Example shown below shows how normalized_shape shall be given (usually just the shape of last few dimensions of input tensor) \n",
    "<center> <img src=\"reference_img/layernormshape.png\" width=600></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.InstanceNorm2d(num_features, eps=1e-5, affine=True, <!-- track_running_stats=True -->...)(input) -> output\n",
    "> input and output have the same shape. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Activations\n",
    "Non-linearity makes neural network capable of approximating any functions, after combining with linear layers. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Below are all common elementwise activation functions, with output shape same as input. \n",
    "- nn.LeakyReLu()(input)\n",
    "- nn.ReLu()(input)\n",
    "- nn.Sigmoid()(input)<img style=\"float:right\" src=\"reference_img/logsig.jpg\" width=300>\n",
    "- nn.LogSigmoid()(input)\n",
    "- nn.Tanh()(input)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.Softmax(dim=None)(input) -> output\n",
    ">  in-place application of softmax activation function, applied on assigned \"dim\" dimension. <br>\n",
    "Note: the dimension to apply should \n",
    "- dim: if \"None\", will apply on flattened tensor; otherwise will apply on assigned dimension. \n",
    "- output: same shape as input. \n",
    "<center><img src=\"reference_img/softmax.jpg\" width=200></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.Softmax2d()(NCHW) -> output\n",
    "> Apply softmax on \"C\" dimension of 2D batched input. \n",
    "\n",
    "> Image below shows the effect: adds values from all channels at each coordinate and do softmax on that: channel_1 + channel_2 + channel_3 == 1, for each [x, y] coordinate in the image/2D tensor. \n",
    "<center><img src=\"reference_img/softmax2d.png\" width=300></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "The beginning of sequential learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary RNN: \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"reference_img/rnn_hidden.png\" width=300><br>\n",
    "weights and biases are initialized along with creation of RNNCell. \n",
    "\n",
    "- “x”: input; \n",
    "- “h”: previous layer’s output; \n",
    "\n",
    "When number of layers is not large, using RNN provides computational efficiency, while the effect of gradient vanishing is not too significant. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity=’tanh’)(input_tensor, hidden_tensor) -> output\n",
    "> Create an RNN cell based on equation described above. \n",
    "- input_size: the size of input_tensor; \n",
    "- hidden_size: the size of hidden_tensor as input; \n",
    "- bias: if false, no bias will be taken into account; \n",
    "- nonlinearity: if ‘relu’, will change activation function to relu instead of \"tanh\" as default activation function\n",
    ">\n",
    "- input_tensor: has shape (N, input_size)\n",
    "- hidden_tensor: has shape(N, hidden_size)\n",
    ">\n",
    "- output: [h'] in formula, has shape (N, hidden_size)\n",
    "An example code shown below gives idea of how previous output along with input shall be used. \n",
    "\n",
    "<br>\n",
    "\n",
    "nn.RNN(input_size, hidden_size, num_layers, nonlinearity=’tanh’, bias=True, dropout=0, bidirectional=False)(input_tensor, previous_h_tensor) -> (Output_tensor, final_hidden_tensor)\n",
    "> Constructs an RNN contains \"num_layers\" many \"RNNCells\", and the connection method follows from description of RNN. \n",
    "- num_layers: how many recurrent layers will present in RNN\n",
    "- dropout: a ratio between 0 and 1; will have a Dropout layer (see later descriptions) for each output with probability set as “dropout”\n",
    "- bidirectional: allows sequence predictions to refer to future prediction results as well. \n",
    ">\n",
    "- input_tensor: has shape (sequence_len, N, input_size); L is sequence length, input_size is # of features; N is batches. \n",
    "- previous_h_tensor(OPTIONAL): in case a complex structure has multiple RNN each with different configuration, then previous RNN’s h_value could be fed as input here. \n",
    "\tIf not provided, will default as zero. \n",
    "\tHas shape (D*num_layers, N, hidden-size); D is 2 if \"bidirectional=True\"\n",
    ">\n",
    "- Output: (Output_tensor, final_hidden_tensor)\n",
    "\t> Has shape [(sequence_len, N, hidden_size), (D*num_layers, N, hidden_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 17])\n",
      "torch.Size([5, 7, 17]) torch.Size([11, 7, 17])\n"
     ]
    }
   ],
   "source": [
    "# RNNcell demo in for loop\n",
    "rnn = nn.RNNCell(13, 17)\n",
    "sequence_len = 5\n",
    "N = 7\n",
    "input = torch.randn(sequence_len, N, 13) # sequence length, batch size, input feature size\n",
    "hidden = torch.randn(N, 17) # batch size, hidden size\n",
    "output = []\n",
    "for i in range (sequence_len):\n",
    "    hidden = rnn(input[i], hidden) # note: hidden is being constantly updated along the way\n",
    "    output.append(hidden)\n",
    "print(output[sequence_len - 1].shape) # batch size, hidden size\n",
    "\n",
    "\n",
    "num_layers = 11\n",
    "rnn_s = nn.RNN(13, 17, num_layers)\n",
    "new_hidden = torch.randn(1 * num_layers, N, 17)\n",
    "rnns_res = rnn_s(input, new_hidden)\n",
    "print(rnns_res[0].shape, rnns_res[1].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"reference_img/lstmcircuit.png\" width=600></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula notations: \n",
    "- h: hidden state; <img style=\"float:right\" src=\"reference_img/lstmfor.png\" width=300>\n",
    "- c: cell state; \n",
    "- x: input; \n",
    "- ifgo: input/forget/cell/output gates; \n",
    "> circle with a dot is element-wise matrix product. \n",
    "\n",
    "Mechanism: when inputs and previous hidden states are given: \n",
    "1. first check forget gates (“f”) and see whether previous hidden states shall be kept or ignored. \n",
    "2. Then “i” and “g” will be calculated, to respectively determine which values to update(by “i”), and how much (by “g”). \n",
    "3. Then “c” is updated based on forget gate and update gate’s contents, which gives current cell’s state. \n",
    "4. Finally “o” is computed, along with next hidden state. New hidden state will take current cell state “c” into account. \n",
    "\n",
    "In general, LSTM allows more manipulation on previous states, so that when analyzing long sequences, only important info would be kept; results in less noise towards understanding current state. Also this resolves “gradient vanishing problems”. \n",
    "However without attention-mechanism, when sequence is long enough LSTM could fail as well\n",
    "\n",
    "To elaborate, the structure of each LSTM cells shown in diagram could be easily modified to have various extra functionings. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.LSTMCell(input_size, hidden_size, bias=True)(input, (hidden, cell)) -> (late_hidden, new_cell)\n",
    "- input: has size (N, input_size)\n",
    "- hidden: has size (N, hidden_size)\n",
    "- cell: has size (N, hidden_size): the initial cell’s state; \n",
    "    > If (hidden, cell) is not provided, will both default to zero tensor. \n",
    ">\n",
    "- late_hidden: has size (N, hidden_size); tensor containing next hidden state\n",
    "- new_cell: has size (N, hidden_size); tensor containing next cell state\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.LSTM(input_size, hidden_size, num_layers, bias=True, dropout=0, bidirectional=False, <br> &emsp;&emsp;proj_size=0)(input, (h0, c0)) -> (output, (hn, cn))\n",
    "- proj_size: if >0,  final output will be linearly projected to this dimension.\n",
    "    > h0, output, hn will have \"hidden_size\" replaced as \"proj_size\". \n",
    "- input: has size (sequence_len, N, input_size)\n",
    "- h0: has size (D*num_layers, N, hidden_size); D=2 if bidirectional\n",
    "- c0: has size (D*num_layers, N, hidden_size)\n",
    ">\n",
    "- output: has size (sequence_len, N, D*hidden_size)\n",
    "- hn: (D*num_layers, N, hidden_size)\n",
    "- cn: has size (D*num_layers, N, hidden_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"reference_img/grudiag.png\" width=400></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:left\" src=\"reference_img/gruform.png\" width=300>\n",
    "\n",
    "- r: reset gate\n",
    "- z: update gate\n",
    "- n: new gate\n",
    "\n",
    "<br>\n",
    "\n",
    "mechanism: \n",
    "1. first calculate update gate(z); this determines how much previous gate’s info shall be passed on; \n",
    "2. Then calculate reset gate(r); this determines how much info shall be forgotten; (in diagram reset gates and update gates are parallel)\n",
    "3. Then new state is calculated;  it represents current cell’s status, containing current input, along with previous hidden states, with a reset ratio. \n",
    "4. Finally new hidden state is calculated based on previous hiddens and current cell status. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.GRUCell(input_size, hidden_size, bias=True)(input, hidden) -> new_hidden\n",
    "- input: has size (N, input_size)\n",
    "- hidden: has size (N, hidden_size)\n",
    ">\n",
    "- new_hidden: has size (N, hidden_size); tensor containing next hidden state\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.GRU(input_size, hidden_size, num_layers, bias=True, dropout=0, bidirectional=False)(input, <br>&emsp;&emsp;h_0) -> (output, h_n)\n",
    "- input: (sequence_len, N, input_size); h_0: (D * num_layers, N, hidden_size); \n",
    "- output: (sequence_len, N, D * hidden_size); h_n: (D * num_layers, N, hidden_size)\n",
    "    > \"D=2\" if \"bidirectional=True\" otherwise \"D=1\". \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of GRU and LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forgetting: \n",
    "    > GRU’s forget/reset gate only handles previous hidden states; \n",
    "\n",
    "    > But LSTM forgets both input and hidden state by a certain rate; \n",
    "\n",
    "- Updating: \n",
    "    > LSTM’s updating and forgetting is applied on COMBINED result of current input and previous hidden state; \n",
    "    \n",
    "    > GRU deals with previous hidden state separately from current input. (see formula of how forget gate and reset gate are calculated for LSTM and GRU for better understanding)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism & Transformer\n",
    "Advanced sequential learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Attention Mechanism and \"nn.MultiHeadAttention\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention mechanism allows the model to \"attend to/focus on\" particular set of information in a long-sequence for making inferences or predictions, instead of allowing all known information to contribute to inference or prediction tasks, which can lead to inaccuracy or bias. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.MultiheadAttention(embed_dim, kdim=None, vdim=None, num_heads, dropout=0.0)(query, key, value) -> (attn_output)\n",
    "> Perform multi-head attention operation on given set of inputs. \n",
    "- embed_dim: query's feature dimension size; \n",
    "- kdim: key's feature dimension size; if \"None\", will be equal to \"embed_dim\". \n",
    "- vdim: value's feature dimension size; if \"None\", will be equal to \"embed_dim\". \n",
    "- num_heads: controls the number of heads for multi-head attention. \n",
    ">\n",
    "- query: (output_sequence_len, N, embed_dim)\n",
    "    > Represents the set of vectors containing acquired information as output. In a sequential setting, 'query' contains previously acquired information, and at each layer of sequential processing, query will be 'refined' with new information(represented by \"key\" and \"value\"). \n",
    "    >\n",
    "    > e.g: in question-answering tasks(NLP), query can be the already existing answer(or Beginning_Of_Sequence, BOS signal), where \"key\" and \"value\" represents the question being asked. \n",
    "- key: (input_sequence_len, N, kdim)\n",
    "    > Represents the new incoming message or already acquired message, used for making inferences on \"query\" acquired from previous sequential processing steps. \n",
    "    >\n",
    "    > Keys will be processed differently than \"value\". For dot-product attention, \"query\" and \"key\" jointly determines the weight assigned to each \"value\" vector for making final weighted sum calculation. \n",
    "- value: (input_sequence_len, N, vdim)\n",
    "    > Also represents the new incoming message or acquired message. \n",
    "    >\n",
    "    > In dot-product attention, calculated weights are applied on \"value\" to acquire updated \"query\" results. \n",
    ">\n",
    "- attn_output: (target_sequence_len, N, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 17])\n"
     ]
    }
   ],
   "source": [
    "# Code demo of attention mechanism, partially referred to personal private slot attention code. \n",
    "class dot_product_attention(nn.Module):\n",
    "    \"\"\"\n",
    "    assumptions for using this module:\n",
    "    the input_shape is assumed to be: (B, D_i) where B is batch, D_i is the input dim;\n",
    "    query shape is assumed to be: (Q, D_s) where Q is number of querys, and D_s is the query feature dim\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, query_shape, attention_dim,\n",
    "                 num_querys, query_mu, query_sigma):\n",
    "\n",
    "        super(dot_product_attention, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.query_shape = query_shape\n",
    "        self.attention_dim = attention_dim\n",
    "        self.q = nn.Linear(query_shape, attention_dim) # attention_dim ensures matrix product shape consistency; \n",
    "        self.k = nn.Linear(input_shape, attention_dim)\n",
    "        self.v = nn.Linear(input_shape, input_shape) # usually the shape is (input_shape, attention)\n",
    "\n",
    "        self.num_querys = num_querys\n",
    "        self.query_mu = query_mu\n",
    "        self.query_sigma = query_sigma\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input: size: [batch, num_input, D], where D is feature dimension\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # first generate querys\n",
    "        querys = self.create_querys(self.query_mu, self.query_sigma,\n",
    "                                  self.num_querys, self.query_shape, input.shape[0])\n",
    "        # perform attention\n",
    "        query = self.q(querys) # shape [..., Q, att_dim]\n",
    "        key = self.k(input) # shape [..., K, att_dim]\n",
    "        value = self.v(input) # shape [..., K, input_dim]\n",
    "        dot_product = query.matmul(key.swapaxes(-1, -2)) * (self.attention_dim ** (-0.5)) # key becomes: [..., att_dim, K] for matrix product operation\n",
    "            # realizing the result is the same as (key.matmul(query.swapaxes(-1, -2))).swapaxes(-1, -2)\n",
    "        # alternative implementation:\n",
    "        # dot_product = torch.einsum(\"bqa, bka -> bqk\", query, key)\n",
    "\n",
    "        weight = nn.Softmax(dim=-1)(dot_product) # realizing \"dot_product\" has shape [..., Q, K],\n",
    "                                                # dim=-1 normalizes \"inputs\" so for each query so the sum of all KEYS'weight for that query is 1\n",
    "        query_prediction = weight.matmul(value) # [..., Q, input_dim]\n",
    "        return query_prediction\n",
    "    \n",
    "    \n",
    "    # create a demo query\n",
    "    def create_querys(self, mu, sigma, k, d_querys, batch_size):\n",
    "        \"\"\"\n",
    "        :param mu: Gaussian sampling mean\n",
    "        :param sigma: Gaussian sampling sigma\n",
    "        :param k: number of querys\n",
    "        :param d_querys: slot dimension\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # will sample querys according to gaussian.\n",
    "        querys = torch.fill(torch.zeros([batch_size, k, d_querys]), mu)\n",
    "        return querys + sigma * torch.randn(querys.shape) # this will return sampled querys\n",
    "\n",
    "dot_prod_module = dot_product_attention(17, 13, 11, 5, 0, 1.0)\n",
    "input = torch.arange(3 * 7 * 17, dtype=torch.float32).reshape(3, 7, 17)\n",
    "print(dot_prod_module(input).shape) # batch of 3, predict 5 queries, each query vector has feature dimension 17. \n",
    "# That is how the weighted sum is being calculated over values. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"reference_img/transformer.png\" width=700></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"reference_img/transformer_proced.png\" width=400></center>\n",
    "<center><img src=\"reference_img/transenc.png\" width=400><img src=\"reference_img/transdec.png\" width=300></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Each encoder’s output will become part of each decoder’s input. </center>\n",
    "\n",
    "Transformer decoder’s input: \n",
    "- Two parts: one is the whole encoder sequence’s output, the other is the previously generated output of whole transformer model (at beginning of translation, a “start” signal is given instead)\n",
    "- transformer uses previously generated outputs and encoder outputs to predict what to generate next one by one, and the new word generated also become part of decoder’s next input. \n",
    "<br><br>\n",
    "<img style=\"float:left\" src=\"reference_img/transformer_mask.png\" width=200>Functionality of masking in decoder: <br>\n",
    "As shown in left image, when predicting next word, the attention score required should only rely on previously generated output’s score. Thus the score for word “fine” is masked out when word “am” is predicted. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Transformer(feature_dim=512, num_heads=8, num_encoder_layer=6, num_decoder_layer=6, dim_feedforward=2048, dropout=0.1, activation=”relu”, layer_norm_eps=1e-05)\n",
    "- feature_dim: expected input features for encoder and decoder. \n",
    "- num_heads: number of heads in multihead attention layer\n",
    "- dim_feedforward: the dimension of feedforward neural network inside each encoder and decoder. \n",
    ">\n",
    "><br>\n",
    "&emsp;(src, tgt, *src_mask, *tgt_mask, *memory_mask) -> output\n",
    "- src: the sequence of encoder input, has shape (encoder_sequence_length, N, embedding_dim)\n",
    "- tgt: the sequence of decoder extra input, has shape (decoder_sequence_length, N, embedding_dim); \n",
    "\t> For natural language processing task, the extra input is usually a signal for start as the first element, and all remaining parts are zero vectors, awaiting for filling in; \n",
    "- src_mask: has shape (encoder_sequence_length, encoder_sequence_length): checks when each input is attended at, which key words shall be ignored from other src input. \n",
    "- tgt_mask: has shape (decoder_sequence_length, decoder_sequence_length)\n",
    "- memory_mask: has shape (DEcoder_sequence_length, ENcoder_sequence_length)\n",
    "\t> Ensures when decoding, which encoder output shall be masked when determining each position of decoder respectively. <br><br>Also indicates dimention 0 is the position to evaluate, and dimension 1 is the words to mask when evaluating dimension 0’s position. \n",
    "\n",
    "- output: has shape (decoder_sequence_length, N, embedding_dim) if “batch_first=False”\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.TransformerEncoderLayer(input_feature, nhead, dim-feedforward=2048, dropout=0.1, activation=”relu”, layer_norm_eps=1e-5)\n",
    "<br> &emsp;(src, src_mask=None)\n",
    "\n",
    "> create an instance of transformer encoder layer as illustrated in figure above. \n",
    "\n",
    "<br>\n",
    "\n",
    "nn.TransformerEncoder(encoder-layer, num-layers, norm=None)\n",
    "<br> &emsp;(src, mask=None)\n",
    "\n",
    "> creates a stack of “num-layers” many transformer encoder layers. <br>\n",
    "\t\tLooks like all stacked encoders are the same, provided by the input parameter. \n",
    "- encoder-layer: an instance of “nn.TransformerEncoderLayer()”. \n",
    "\n",
    "<br>\n",
    "\n",
    "nn.TransformerDecoderLayer(input_feature, nhead, dim-feedforward=2048, dropout=0.1, activation=”relu”, layer_norm_eps=1e-5)\n",
    "<br> &emsp;(tgt, memory, tgt_mask=None, memory_mask=None)\n",
    "\n",
    "> Create a decoder layer for transformer as described in previous diagrams\n",
    "- memory: the result from FINAL encoder layer ONLY; \n",
    "- tgt: realizing the decoder layer might not be the first layer; thus apart from beginning of “tgt sequence”, other elements in the “tgt sequence” might also be non-zero; \n",
    "\n",
    "<br>\n",
    "\n",
    "nn.TransformerDecoder(decoder-layer, num_layers, norm=None)\n",
    "<br> &emsp;(tgt, memory, tgt_mask=None, memory_mask=None)\n",
    "\n",
    "> Creates a stack of transformer decoder layers. \n",
    "- decoder-layer: an instance of the “TransformerDecoderLayer()”\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout & Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Dropout layers are used to help improving independence between input layers, which can prevent model from overfitting. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Dropout2d(p=0.5, inplace=False)(input) -> output\n",
    "> mask the entire channel given a \"input\" with shape \"(NCHW)/(CHW)\" \n",
    "<br>&emsp;When the channel “C” is chosen, input[:C:…] will be filled by 0. \n",
    "\n",
    "- p: the probability of masking the given channel, following a Bernoulli distribution\n",
    "- input: has shape (NCHW)/(CHW) depending on input shape\n",
    "- output: has same shape as input\n",
    "> Image below shows how channel-wise dropout is conducted. \n",
    "<center><img src=\"reference_img/dropout2dcha.png\" width=200></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.Dropout(p=0.5, inplace=False)(input) -> output\n",
    "> mask the input entries randomly by a probability of “p”. \n",
    "<br>&emsp;compared with Dropout2d, the masking is irregular, and not restricted to channel\n",
    "\n",
    "- input: a tensor with no restrictions on shape\n",
    "- output: has same shape as input\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.AlphaDropout(p=0.5, inplace=False)(input) -> output\n",
    "> randomly mask a value with probability “p”, and MAINTAIN mean and std of data. (specific details require reading papers)\n",
    "- \"input\" and \"output\" must have the same shape.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)(input_tensor) -> output\n",
    "- num_embeddings: the number of elements to be indexed by embedding; \n",
    "- embedding_dim: the number of features of each embedded vector; \n",
    "- padding_idx: the integer for padding the remaining space of input -> ensure input length consistency for those short inputs. \n",
    "- max_norm: a restriction on embedding vector’s norm, to ensure consistency of vector scale. \n",
    "- norm_type: the p-scale (specific restriction on the norm of embedding vector)\n",
    "- scale_grad_by_freq: if True, will scale gradients (in backward pass) in INVERSE of word’s frequency\n",
    "><br>\n",
    "- input_tensor: can have any shape; \n",
    "- output: have shape (input_tensor_shape, embedding_dim) -> the set of embedded vectors for all input components\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips on using Embedding: \n",
    "Refer to code shown left; \n",
    "- realizing that embedding tries to embed EACH ELEMENT from the input tensor; thus, for example, taking in a number of words/strings, the first step is to iteratively assign each string a number, then input the numbers into embedding function to acquire a translated result for training models. \n",
    "- the number can be acquired using a dictionary, mapping each string to a number; the efficient way to create such dictionary is: <br>\n",
    "&emsp;using “zip()” function in python; the code shown right uses the function to combine two length-consistent lists into a pair-wise dictionary. <br>\n",
    "&emsp;When “dict()”ing the zipped result, the duplicated words will be handled automatically; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(10, 4)\n",
    "a = torch.tensor([1, 0, 1, 0])\n",
    "print(embed(a)) # note 1st and 3rd row are the same -> duplicates handling\n",
    "a = torch.tensor([1, 0, 1, 0, 2, 3, 4])\n",
    "print(embed(a)) # note the first 4 rows is same as previous output -> embedding is set for each nn.Embedding module;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo of zip() function, and potentially tokenizing an input sentence, useful for NLP tasks\n",
    "test_string = \"this is just a test sentence that makes totally no sense, yes it is\"\n",
    "all_words = test_string.split()\n",
    "indexing = torch.arange(len(all_words))\n",
    "zip_res = dict(zip(all_words, indexing)) # this ensures the key of dictionary is \"all_words\", and values are \"indexing\"\n",
    "print(zip_res) # realize the word \"it\" occurs for only one time in resulting dict, showing duplicates handling\n",
    "# convert the test string into tensor representation. \n",
    "# perhaps there is a better way than using for loop...?\n",
    "res = []\n",
    "for word in all_words:\n",
    "    res.append(zip_res[word])\n",
    "print(torch.tensor(res)) # the resulting format can be applied with nn.Embedding, following previous chunk of code\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function: \n",
    "Will only list commonly used ones, and more choices can be found in pytorch API. <br>\n",
    "Custom loss functions is also possible to define, so long as \"input\" and \"target\" shape is preserved like other loss functions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.L1Loss(reduction='mean')(input, target) -> output\n",
    "> Perform L1Loss on \"input\" and \"target\". \n",
    "<center><img src=\"reference_img/l1loss.png\" width=300></center>\n",
    "\n",
    "- reduction: if 'mean', will take the average of all losses; if 'sum', will sum-up all the acquired losses; if 'None', no further operations will be performed, and \"output\" has same shape as \"input\". \n",
    "><br>\n",
    "- input, target: must have same shape\n",
    "- output: a scalar or a tensor with same shape as \"input\". \n",
    "    > usually preferred to be a scalar for performing \"output.backward()\", weight update operation\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.MSELoss(reduction='mean')(input, target) -> output\n",
    "> Perform mean squared error loss calculation on \"input\" and \"target\".\n",
    "<center><img src=\"reference_img/mse.png\" width=400></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "nn.CrossEntropyLoss(reduction='mean')(input, target) -> output\n",
    "> Perform cross entropy loss on input and target\n",
    "<center><img src=\"reference_img/crossentropy.png\" width=500></center>\n",
    "\n",
    "- input: has shape (N, C) or (N, C, d1, ..., dk); N: batch size; C: number of classes; d1, ... dk: dimension of inputs; \n",
    "- target: has shape (N) or (N, d1, ..., dk)\n",
    "- output: either a scalar or: has shape (N) or (N, d1, ..., dk). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operation Related: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets & Dataloaders\n",
    "\n",
    "\"import torch.utils.data as data_p\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below demonstrates how to construct a dataset, can be as simple as just overwritting \n",
    "# 3 functions: __init__, __len__, __getindex__, and other helper functions. \n",
    "# realizing above 3 functions are abstract methods that MUST be re-written. \n",
    "\n",
    "import torch.utils.data as data_p\n",
    "\n",
    "class SampleDataset(data_p.Dataset): # all datasets need to inherit their abstract class: torch.utils.data.Dataset\n",
    "    def __init__(self, data, target):\n",
    "        \"\"\"\n",
    "        data: each single element from the \"sequential data\" has dimension [sequence_length, feature_size].\n",
    "        target: has dimension [sequence_length]\n",
    "        \"\"\"\n",
    "        super(SampleDataset, self).__init__()\n",
    "        self.sequential_data = data\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This method is required to overwrite, as its superclass doesn't implement this method\n",
    "        This method simply returns how many pairs of data-target (or data piece, for unsupervised learning) are stored within this dataset. \n",
    "        \"\"\"\n",
    "        return len(self.sequential_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        This method is required to overwrite, as its superclass doesn't implement this method\n",
    "        Return the (data, target) pair (or just data, for unsupervised learning) at specific index. \n",
    "        \n",
    "        index: the INDEX of data required to extract\n",
    "        \"\"\"\n",
    "        return self.sequential_data[index], self.target[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elaborations: \n",
    "1. Do not underestimate the flexibility of dataset initialization methods. To read data, there are several ways: \n",
    "    - Directly load as tensor and feed as input parameters (above code)\n",
    "    - Provide \"PATH\" to the data folder, and pass in \"PATH\" as parameter, instead of \"data\" or \"target\". <br> Then inside \"\\_\\_init__\", use \"read()\" method and further processing to convert data into appropriate format. \n",
    "2. Initialization of Dataset can also provide additional functionalities. \n",
    "    - All vision datasets allows inputting a sequence of preprocessing methods (see \"torchvision.transforms\" for more details) for images; \n",
    "3. \"\\_\\_getitem__\" method can actually perform preprocessing on data pieces before returning for training. \n",
    "    - e.g: vision data pieces could be applied with \"torchvision.transforms\" before returning. \n",
    "4. \"\\_\\_getitem__\" doesn't have restriction on the format of returned data, so long as follow-up code extracts returned data pieces appropriately. \n",
    "\n",
    "In general, explore how you could load data in various ways, and play with those data pieces in whichever way you like before returning them. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of dataloader method\n",
    "# do not run the code below\n",
    "import math\n",
    "\n",
    "dataset = SampleDataset(data, target)\n",
    "\n",
    "# train-test split\n",
    "train_test_factor = 0.8 # 80% training, 20% testing; \n",
    "train_size = math.ceil(len(dataset) * train_test_factor)\n",
    "split_dataset = data_p.random_split(dataset, [train_size, len(dataset) - train_size]) # explanation of this method is in markdown part below\n",
    "\n",
    "train_dataloader = data_p.DataLoader(split_dataset[0], batch_size=50) # This is how dataloaders are initialized, in simplest ways. \n",
    "valid_dataloader = data_p.DataLoader(split_dataset[1], batch_size=1)\n",
    "\n",
    "# the following lines of code shows how to extract data from DataLoaders containing \"SampleDataset\" format data. \n",
    "# pay attention to \"index\", and realize \"sample\" is the return value of \"__getitem__\" from \"SampleDataset\". \n",
    "for index, sample in enumerate(train_dataloader):\n",
    "    data, target = sample[0], sample[1] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_p.DataLoader(dataset, batch_size=1, shuffle=None, sampler=None, num_workers=0, ...)\n",
    "> Initializes a \"DataLoader\" class for given \"dataset\". \n",
    "- dataset: subclasses of class \"data_p.Dataset\"\n",
    "- shuffle: if 'True', data will be reshuffled at every __epoch__. Otherwise data will be reshuffled only once for entire program running. \n",
    "- sampler: a subclass of \"data_p.Sampler\", will explain later. \n",
    "- num_workers: if '>0', data loading process will be conducted in multiple processors. Useful when using multiple GPU/TPU to load data and train. \n",
    "\n",
    "<br>\n",
    "\n",
    "data_p.random_split(dataset, lengths) -> List[dataset]\n",
    "> Randomly split the given dataset into a list of multiple __non-overlapping__ subsets of \"data_p.Dataset\" object. \n",
    "- dataset: a subclass of \"data_p.Dataset\"\n",
    "- lengths: \n",
    "    1. if a list of integers summing up to \"len(dataset)\", will split \"dataset\" into \"len(lengths)\" many datasets, each with length in \"lengths\". \n",
    "    2. if a list of non-negative fractions summing up to \"1\", will split \"dataset\" into propotional segments. \n",
    "\n",
    "<br>\n",
    "\n",
    "data_p.Sampler(data_source, ...)\n",
    "> Initialize a data sampler. It's the abstract class of all Samplers. \n",
    "- \"\\_\\_iter__\": the method all Sampler subclasses must overwrite, to provide a method for iterating over the indices of \"data_source\"\n",
    "- \"\\_\\_len__\": another method all Sampler subclasses must overwrite. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validating\n",
    "This section will mainly consist of code and explanation comments. Additional references will be listed at end of code. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "NUM_BATCHES = 200 # batch_size\n",
    "EPOCHS = 100 # number of total training epoches;\n",
    "TRAIN_TEST_SPLIT = 0.9 # divide dateset into training and testing sets\n",
    "EARLY_STOP_THRESHOLD = 10 # See below explanation for early stopping\n",
    "\n",
    "lr_milestones = [17, 40, 75] # learning rate milestones; see below description\n",
    "lr_decay_gamma = 0.5 # learning rate decay ratio\n",
    "loss_fn = torch.nn.MSELoss() # loss function, here is mean squared error\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # device setting. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key words explanation: \n",
    "1. Epoches: determines how many total iterations over entire training dataset is required. \n",
    "    - Each epoch requires iterating through all training data in a batch-wise manner, and then calculate total loss for performing backpropagation, updating weights. \n",
    "2. Early Stopping: a technique for avoid overfitting. \n",
    "    - If the validation loss hasn't decrease for consecutive __EARLY_STOP_THRESHOLD__ many epoches, the training will stop and adopt the model weight with lowest validation loss as final weight of trained model. \n",
    "3. Learning rate decay: during training, as loss becomes lower and lower, learning rate also need to decrease for more slight improvements of weights, and to avoid overshooting local minimum. See image below for why learning rate decay is necessary: as training loss becomes closer and closer to a minimum, the required adjustments to weights should also be smaller and smaller.  \n",
    "    - lr_milestones: determines \"epoches\" where learning rate should decay. \n",
    "    - lr_decay_gamma: new learning rate will change at lr_milestones, by multiplying with \"lr_decay_gamma\". \n",
    "<center><img src=\"reference_img/lr_decay.jpg\" width=400></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, device):\n",
    "    model.to(device)\n",
    "    model.train() # set model status to train to adjust model's settings. \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # set up optimizer methods. See below for more references. Here Adam method is used\n",
    "\n",
    "    # learning rate scheduler:\n",
    "    lr_schedule = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                lr_milestones, gamma=lr_decay_gamma) # set up learning rate scheduling, see below for more scheduling methods\n",
    "\n",
    "    # early stopping:\n",
    "    curr_best = model.state_dict() # see \"state_dict\" referenced below. \n",
    "    curr_lowest_val_accuracy = math.inf \n",
    "    stop_count = 0 # for checking with EARLY_STOP_THRESHOLD\n",
    "\n",
    "    for iterations in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        prediction, actual = (None, None)\n",
    "\n",
    "        optimizer.zero_grad() # This step ensures gradients are reset for each epoch. Otherwise gradients update will accumulate, leading to errors. \n",
    "\n",
    "        for index, sample in enumerate(train_dataloader):\n",
    "            device_sample, device_target = sample[0].to(device), sample[1].to(device) # feed data and target to device\n",
    "            result = model(device_sample) # acquire model result\n",
    "            prediction = result[0] # acquiring prediction results depends on the output of the model!!!\n",
    "            actual = device_target[0]\n",
    "            loss = loss_fn(result, device_target) # acquire loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward() # loss update is done batch-wise; \n",
    "            optimizer.step()\n",
    "        lr_schedule.step() # check learning rate and decide to decrease learning rate or not\n",
    "        average_loss = total_loss / NUM_BATCHES\n",
    "        print(\"epoch {}: average training loss is ({})\".format(iterations, average_loss))\n",
    "\n",
    "        # validation & early stopping\n",
    "        validation_accuracy = validate(model, valid_dataloader, device)\n",
    "        model_saved = False\n",
    "        if validation_accuracy < curr_lowest_val_accuracy:\n",
    "            curr_lowest_val_accuracy = validation_accuracy\n",
    "            curr_best = model.state_dict()\n",
    "            stop_count = 0\n",
    "            model_saved = True\n",
    "        else:\n",
    "            stop_count += 1\n",
    "            if stop_count == EARLY_STOP_THRESHOLD:\n",
    "                print(\"reached early stopping threshold, training stopped\")\n",
    "                break\n",
    "        print(\"early_stopping_watcher: stop count: {}; model saved: {}\\n\".format(stop_count, model_saved))\n",
    "        # print(\"prediction: {}; actual: {}\".format(prediction, actual))\n",
    "    # save model\n",
    "    torch.save(curr_best, \"../model/model_state.pt\") # save model weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarized procedure: \n",
    "1. check device setting for \"model\" and \"data\". \n",
    "2. Setting up optimization method and learning rate scheduling. \n",
    "3. In epoch \"for-loop\": iterate over batches of datasets, calculate loss and perform gradient updates FOR EACH batch!!!\n",
    "4. Validate model using validation datasets\n",
    "5. Update learnin rate, collect total loss, determine early stopping. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers: \n",
    "For a complete reference of each optimizer, visit: <br>\n",
    "https://pytorch.org/docs/stable/optim.html\n",
    "<br><br>\n",
    "\"import torch.optim as optim\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optim.Optimizer()\n",
    "> Abstract class inherited by all below optimizers\n",
    "\n",
    "<br>\n",
    "\n",
    "optim.Adam(params, lr=0.001, eps=1e-08, weight_decay=0, ...)\n",
    "> Initializes adam optimizer\n",
    "- params: usually \"model.parameters()\", the initial/already updated weights of the model. \n",
    "- lr: learning rate\n",
    "- eps: numeric stable terms for denominators\n",
    "- weight_decay: L2 penalty, for weight regularizer, a method preventing overfitting\n",
    "\n",
    "<br>\n",
    "\n",
    "optim.SGD(params, lr, momentum=0, dampening=0, weight_decay=0)\n",
    "> Initializes optimizer adopting stochastic gradient descent method, potentially with momentum. \n",
    "\n",
    "> Momentum: the formula below shows its effect: previous step's gradient also contributes to current step's gradient, to prevent the situation shown in 3rd image below. \n",
    "\n",
    "<center><img src=\"reference_img/sgdmomentum.png\" width=300>  <img src=\"reference_img/sgdmomevec.png\" width=500>  <img src=\"reference_img/sgdmomeloss.png\" width=400></center>\n",
    "\n",
    "- momentum: \"beta\" in above formula\n",
    "- dampening: controls the reduction of momentum as training progresses. (a ratio between 0 and 1)\n",
    "\n",
    "<br>\n",
    "\n",
    "optim.Adadelta/Adagrad/AdamW/SparseAdam/Adamax/ASGD/LBFGS/NAdam/RAdam/RMSprop/Rprop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate scheduler:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1, verbose=False)\n",
    "> update learning rate by multiplying previous learning rate by a given function \"lr_lambda\". \n",
    "- optimizer: a subclass of optim.Optimizer()\n",
    "- lr_lambda: a __function__ computing a multiplicative factor for current epoch. \n",
    "    > input of function: current epoch(an integer)\n",
    "- last_epoch: the index of __previous__ epoch; \"-1\" indicates training hasn't started yet. \n",
    "- verbose: if \"True\", prints out learning rate update information for each update. \n",
    "\n",
    "<br>\n",
    "\n",
    "optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda, last_epoch=-1, verbose=False)\n",
    "> Quite similar to LambdaLR. \n",
    "\n",
    "<br>\n",
    "\n",
    "optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "> decays learning rate at given \"step_size\" periods by multiplying with \"gamma\". \n",
    "- step_size: an integer, representing periodic benchmark; when reached, will update current learning rate by multiplying with \"gamma\". \n",
    "\n",
    "<br>\n",
    "\n",
    "optim.lr_scheduler.StepLR(optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "> compared with \"StepLR\", \"milestones\" can be a list of INCREASING integers, and updates of learning rate doesn't need to be periodic. \n",
    "\n",
    "<br>\n",
    "\n",
    "optim.lr_scheduler.LinearLR(optimizer, start_factor=0.3333333333333333, end_factor=1.0, total_iters=5, last_epoch=-1, verbose=False)\n",
    "> Multiplies current learning rate with a __linearly increasing__ factor grows from \"start_factor\" to \"end_factor\" evenly for \"total_iters\" many epoches.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OUTPUT = 6\n",
    "VALIDATION_THRESHOLD = 0.25\n",
    "\n",
    "def validate(model, valid_dataloader, device):\n",
    "    \"\"\"\n",
    "    if the predicted price is within a certain range, will treat the prediction as correct.\n",
    "    threshold is a hyperparameter set at beginning of file.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # for GPU memory saving. \n",
    "        correct_prediction = [0] * NUM_OUTPUT # all updates for correct predictions are used to calculate accuracy. \n",
    "        total_predictions = 0 \n",
    "        total_loss = 0\n",
    "\n",
    "        for index, sample in enumerate(valid_dataloader):\n",
    "\n",
    "            # chunk below acquires the result of prediction and target\n",
    "            device_sample, device_target = sample[0].to(device), sample[1].to(device)\n",
    "            result = model(device_sample)\n",
    "            prediction = result\n",
    "            actual = device_target[0]\n",
    "            total_predictions += 1 \n",
    "\n",
    "            # chunk below is used for updating data for calculating validation accuracy\n",
    "            # for this particular model and task, a prediction is considered \"correct\" if the predicted result falls in a \n",
    "            for period in range(len(prediction)):\n",
    "                actual_low = actual[period] - VALIDATION_THRESHOLD\n",
    "                actual_high = actual[period] + VALIDATION_THRESHOLD\n",
    "                if actual_low <= prediction[period] <= actual_high:\n",
    "                    correct_prediction[period] += 1\n",
    "            \n",
    "            # chunk below updates loss\n",
    "            loss = loss_fn(prediction, actual)\n",
    "            total_loss += loss\n",
    "        \n",
    "        # print_statement should be modified;\n",
    "        accuracy = []\n",
    "        for i in range(len(correct_prediction)):\n",
    "            accuracy.append(round((correct_prediction[i] / total_predictions), 2)) # this is the formula for calculating validation accuracy\n",
    "        average_loss = total_loss / index\n",
    "        print(\"average validation loss: \" + str(average_loss.item()))\n",
    "        print(\"number of in-threshold predictions: \" + str(accuracy))\n",
    "        print(\"number of total predictions: \" + str(total_predictions))\n",
    "\n",
    "    # calculate evaluation score, based on a weight measure emphasizing long term prediction\n",
    "    return average_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of validation step: \n",
    "1. \"with torch.no_grad()\": reduce GPU usage, as gradient update is not required. \n",
    "2. no longer updates gradients, so no optimizers. \n",
    "3. validation loss's calculation method is the same as training loss calculation. \n",
    "4. calculating validation accuracy is actually optional. Usually accuracy is calculated as the number of correct predictions over total predictions. <br> Depending on the tasks, there could be several ways of evaluating whether one prediction is correct or not: \n",
    "    - Regression (the code above): a threshold allowing predictions to be deviating around a reasonable range. \n",
    "    - Classification: the likelihood of the correct class is the highest among all other classes' predictions. (take \"argmax\")\n",
    "    - Other custom methods to determine whether a prediction is correct or not. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "631eef1b4b52ed6a89046ff369727979bce7989ab69f0654255b8572dc17497a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
